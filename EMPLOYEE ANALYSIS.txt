# Importing the necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

# Importing the csv file
data = pd.read_csv('Hr.csv')
data.shape
data.columns
data.head()
#Looking for missing data
data.info()

# A new pandas Dataframe is created to analyze department wise performance as asked.
dept = data.iloc[:,[5,27]].copy()
dept_per = dept.copy()
# Finding out the mean performance of all the departments and plotting its bar graph using barplot.
dept_per.groupby(by='EmpDepartment')['PerformanceRating'].mean()
plt.figure(figsize=(10, 4.5))
sns.barplot(x='EmpDepartment', y='PerformanceRating', data=dept_per)
plt.show()
#Analyze each department separately
dept_per.groupby(by='EmpDepartment')['PerformanceRating'].value_counts()
#Creating a new dataframe to analyze each department separately
department = pd.get_dummies(dept_per['EmpDepartment'])
performance = pd.DataFrame(dept_per['PerformanceRating'])
dept_rating = pd.concat([department,performance],axis=1)
# Plotting a separate bar graph for performance of each department using seaborn
plt.figure(figsize=(15,10))

plt.subplot(2,3,1)
sns.barplot(x='Sales',y='PerformanceRating',data=dept_rating)

plt.subplot(2,3,2)
sns.barplot(x='Development',y='PerformanceRating',data=dept_rating)

plt.subplot(2,3,3)
sns.barplot(x='Research & Development',y='PerformanceRating',data=dept_rating)

plt.subplot(2,3,4)
sns.barplot(x='Human Resources',y='PerformanceRating',data=dept_rating)

plt.subplot(2,3,5)
sns.barplot(x='Finance',y='PerformanceRating',data=dept_rating)

plt.subplot(2,3,6)
sns.barplot(x='Data Science',y='PerformanceRating',data=dept_rating)
plt.show()

# Encoding all the ordinal columns and creating a dummy variable for them to see if ther
enc = LabelEncoder()
for i in (2,3,4,5,6,7,16,26):
 data.iloc[:,i] = enc.fit_transform(data.iloc[:,i])
data.head()
#Finding out the correlation coeffecient to find out which predictors are significant.
data.corr()
#Dropping the first columns as it is of no use for analysis.
data.drop(['EmpNumber'],inplace=True,axis=1)
data.head()
# Here we have selected only the important columns
y = data.PerformanceRating
#X = data.iloc[:,0:-1] 
#All predictors were selected it resulted in dropping of accuracy
# Taking only variables with correlation coef
x = data.iloc[:,[4,5,9,16,20,21,22,23,24]] 
x.head()
#Splitting into train and test for calculating the accuracy
X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=10)
#Standardization technique is used
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
X_train.shape
X_test.shape


#Training the model
from sklearn.ensemble import RandomForestClassifier

#Create an instance of the Random Forest classifier with specified parameters:
classifier_rfg=RandomForestClassifier(random_state=33,n_estimators=23)
parameters=[{'min_samples_split':[2,3,4,5],'criterion':['gini','entropy'],'min_samples_leaf':[1,2,3]}]
#Create an instance of GridSearchCV and fit the data:
model_gridrf=GridSearchCV(estimator=classifier_rfg, param_grid=parameters, scoring='accuracy')
model_gridrf.fit(X_train,y_train)

model_gridrf.best_params_
# Predicting the model
y_predict_rf = model_gridrf.predict(X_test)
#Finding accuracy, precision, recall and confusion matrix

print(accuracy_score(y_test,y_predict_rf))
print(classification_report(y_test,y_predict_rf))
confusion_matrix(y_test,y_predict_rf)

